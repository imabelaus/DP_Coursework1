{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_NLP_Imanol_Belausteguigoitia.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LztpZefvykd",
        "colab_type": "text"
      },
      "source": [
        "## **Summarizing Key Data Science Concepts**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuYb5cx5vw_O",
        "colab_type": "text"
      },
      "source": [
        "Text summarization is a subject of Natural Language Processing (NLP) that extract extracting summaries from large texts. I think it can be useful for many themes such as academic literature summarization, novels, or even medical books. According to what I reseached, there are maily two techniques for text summarization: deep learning-based techniques and simple NLP techniques. This project explores simple NLP-based technique to extract key data science concepts from Wikipedia. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCuX6zd_K0o-",
        "colab_type": "text"
      },
      "source": [
        "**Importing relevant packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYXNfzZzaTSE",
        "colab_type": "code",
        "outputId": "d0c876fa-e387-4589-865c-961a5231e48d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "import nltk\n",
        "import bs4 as bs  \n",
        "import urllib.request  \n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgex_bc6yfXr",
        "colab_type": "text"
      },
      "source": [
        "**Workflow**\n",
        "\n",
        "**1. Converting paragraphs into sentences**\n",
        "\n",
        "**2. Text Preprocessing**\n",
        "\n",
        "a) Tokenizing the Sentences\n",
        "In order to pre process text we will need to tokenize all the sentences to get all the words that exist in the sentences. \n",
        "\n",
        "b) Weighted Frequency of Occurrence\n",
        "This step is crucial, we will determine the weighted frequency of each word by dividing its frequency by the frequency of the most repeated word.  Bear in mind that it is important to remove stopwords.\n",
        "\n",
        "\n",
        "c) Replace Weighted Frequency in Original Sentences\n",
        "The final step is to plug the weighted frequency in place of the corresponding words in original sentences and finding their sum.\n",
        "\n",
        "**3. Sort  sentences in descending order**\n",
        "\n",
        "The sentences with highest frequencies summarize the text, in this case, I will take into consideration 7 sentences. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RkSHq9BwA_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Machine_learning')  \n",
        "article = scraped_data.read()\n",
        "\n",
        "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
        "\n",
        "paragraphs = parsed_article.find_all('p')\n",
        "\n",
        "article_text = \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXIdz7YDbaie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for p in paragraphs:  \n",
        "    article_text += p.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5JFdfRLvXVX",
        "colab_type": "text"
      },
      "source": [
        "**Pre processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOgWEbn_bfDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PREPROCESSING: REMOVING STRANGE CHARACTERS\n",
        "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)  \n",
        "article_text = re.sub(r'\\s+', ' ', article_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86IUNeAUbfGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PREPROCESSING\n",
        "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  \n",
        "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbn2kdXEMskH",
        "colab_type": "text"
      },
      "source": [
        "**Tokenizing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX6YXMn3bfJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_list = nltk.sent_tokenize(article_text)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-wOmLuLMwYd",
        "colab_type": "text"
      },
      "source": [
        "**Removing stop words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBK2QOVFbfPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "word_frequencies = {}  \n",
        "for word in nltk.word_tokenize(formatted_article_text):  \n",
        "    if word not in stopwords:\n",
        "        if word not in word_frequencies.keys():\n",
        "            word_frequencies[word] = 1\n",
        "        else:\n",
        "            word_frequencies[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYNMO6iUM310",
        "colab_type": "text"
      },
      "source": [
        "**Bulding the most repeated word algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udWTkXCBbfSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maximum_frequncy = max(word_frequencies.values())\n",
        "\n",
        "for word in word_frequencies.keys():  \n",
        "    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTiCBxsCcDeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_scores = {}  \n",
        "for sent in sentence_list:  \n",
        "    for word in nltk.word_tokenize(sent.lower()):\n",
        "        if word in word_frequencies.keys():\n",
        "            if len(sent.split(' ')) < 30:\n",
        "                if sent not in sentence_scores.keys():\n",
        "                    sentence_scores[sent] = word_frequencies[word]\n",
        "                else:\n",
        "                    sentence_scores[sent] += word_frequencies[word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46JRe0dTM_aV",
        "colab_type": "text"
      },
      "source": [
        "**Result!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8un3th_ecDgy",
        "colab_type": "code",
        "outputId": "8f8acd04-efb4-45f1-a8e4-0a5cec1962d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import heapq  \n",
        "summary_sentences = heapq.nlargest(10, sentence_scores, key=sentence_scores.get)\n",
        "\n",
        "machine_learning = ' '.join(summary_sentences)  \n",
        "print(machine_learning)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems. Data mining is a field of study within machine learning, and focuses on exploratory data analysis through unsupervised learning. Unsupervised learning algorithms take a set of data that contains only inputs, and find structure in the data, like grouping or clustering of data points. Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model, wherein \"algorithmic model\" means more or less the machine learning algorithms like Random forest. The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. A central application of unsupervised learning is in the field of density estimation in statistics, though unsupervised learning encompasses other domains involving summarizing and explaining data features. The neural network itself is not an algorithm, but rather a framework for many different machine learning algorithms to work together and process complex data inputs.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0w5D-Ec7Uhw",
        "colab_type": "text"
      },
      "source": [
        "### Replicate the process to get more key concepts from DATA SCIENCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY1XC1dZ7Rdo",
        "colab_type": "text"
      },
      "source": [
        "LET'S DO A TOTAL OF 7  MORE KEYS WORDS:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY0OxTwP4-sL",
        "colab_type": "text"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "- No need to re run \n",
        "\n",
        "- BUT IF YOU WANT TO TEST:\n",
        "\n",
        "1. Run one of the scrape_data links\n",
        "2. Run the \"article......get)\" command line\n",
        "3. Run the relevant command line. \n",
        "\n",
        "(Example: \n",
        "\n",
        "for --> scraped_data = urllib.request.urlopen ('https://en.wikipedia.org/wiki/P-value')\" \n",
        "\n",
        "run   --> P_value = ' '.join(summary_sentences)  \n",
        "print(P_value))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nidybAny0oNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/P-value')  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijOQJ4X73lI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Logistic_regression')  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2H_TkQ53xsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Linear_regression')  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpSQXG-J3xxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Cluster_analysis')  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9pZKC893yBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Principal_component_analysis')  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6SwlVT74ZTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Algorithm')  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiDjb97u4ZfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Neural_network')  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KFFMEBd6C5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "article = scraped_data.read()\n",
        "\n",
        "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
        "\n",
        "paragraphs = parsed_article.find_all('p')\n",
        "\n",
        "article_text = \"\"\n",
        "\n",
        "for p in paragraphs:  \n",
        "    article_text += p.text\n",
        "    \n",
        "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)  \n",
        "article_text = re.sub(r'\\s+', ' ', article_text)\n",
        "\n",
        "# Removing special characters and digits\n",
        "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  \n",
        "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
        "\n",
        "sentence_list = nltk.sent_tokenize(article_text) \n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "word_frequencies = {}  \n",
        "for word in nltk.word_tokenize(formatted_article_text):  \n",
        "    if word not in stopwords:\n",
        "        if word not in word_frequencies.keys():\n",
        "            word_frequencies[word] = 1\n",
        "        else:\n",
        "            word_frequencies[word] += 1\n",
        "\n",
        "maximum_frequncy = max(word_frequencies.values())\n",
        "\n",
        "for word in word_frequencies.keys():  \n",
        "    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
        "\n",
        "sentence_scores = {}  \n",
        "for sent in sentence_list:  \n",
        "    for word in nltk.word_tokenize(sent.lower()):\n",
        "        if word in word_frequencies.keys():\n",
        "            if len(sent.split(' ')) < 30:\n",
        "                if sent not in sentence_scores.keys():\n",
        "                    sentence_scores[sent] = word_frequencies[word]\n",
        "                else:\n",
        "                    sentence_scores[sent] += word_frequencies[word]\n",
        "\n",
        "\n",
        "import heapq  \n",
        "summary_sentences = heapq.nlargest(10, sentence_scores, key=sentence_scores.get)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXslzzBW6lwW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ff74a60e-dbd9-494c-94ce-405b1070c3ba"
      },
      "source": [
        "P_value = ' '.join(summary_sentences)  \n",
        "print(P_value)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thus computing a p-value requires a null hypothesis, a test statistic (together with deciding whether the researcher is performing a one-tailed test or a two-tailed test), and data. For example, AMA style uses \"P value,\" APA style uses \"p value,\" and the American Statistical Association uses \"p-value.\" The rejection of the null hypothesis implies that the correct hypothesis lies in the logical complement of the null hypothesis. For typical analysis, using the standard α = 0.05 cutoff, the null hypothesis is rejected when p < .05 and not rejected when p > .05. However, unless there is a single alternative to the null hypothesis, the rejection of null hypothesis does not tell us which of the alternatives might be the correct one. Fisher also underlined the interpretation of p, as the long-run proportion of values at least as extreme as the data, assuming the null hypothesis is true. The null hypothesis is that the coin is fair, and the test statistic is the number of heads. As such, the test statistic follows a distribution determined by the function used to define that test statistic and the distribution of the input observational data. The p-value is widely used in statistical hypothesis testing, specifically in null hypothesis significance testing. In modern terms, he rejected the null hypothesis of equally likely male and female births at the p = 1/282 significance level.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAUPg6DT7C_7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "56280524-546c-4edf-acee-449b89382305"
      },
      "source": [
        "Feature_selection = ' '.join(summary_sentences)  \n",
        "print(Feature_selection)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thus computing a p-value requires a null hypothesis, a test statistic (together with deciding whether the researcher is performing a one-tailed test or a two-tailed test), and data. For example, AMA style uses \"P value,\" APA style uses \"p value,\" and the American Statistical Association uses \"p-value.\" The rejection of the null hypothesis implies that the correct hypothesis lies in the logical complement of the null hypothesis. For typical analysis, using the standard α = 0.05 cutoff, the null hypothesis is rejected when p < .05 and not rejected when p > .05. However, unless there is a single alternative to the null hypothesis, the rejection of null hypothesis does not tell us which of the alternatives might be the correct one. Fisher also underlined the interpretation of p, as the long-run proportion of values at least as extreme as the data, assuming the null hypothesis is true. The null hypothesis is that the coin is fair, and the test statistic is the number of heads. As such, the test statistic follows a distribution determined by the function used to define that test statistic and the distribution of the input observational data. The p-value is widely used in statistical hypothesis testing, specifically in null hypothesis significance testing. In modern terms, he rejected the null hypothesis of equally likely male and female births at the p = 1/282 significance level.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf_gWDfi7C4Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a14737ea-93a3-49b7-bbf5-0a76bcae5314"
      },
      "source": [
        "Logistic_regression = ' '.join(summary_sentences)  \n",
        "print(Logistic_regression)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function, which is the cumulative logistic distribution. The model of logistic regression, however, is based on quite different assumptions (about the relationship between dependent and independent variables) from those of linear regression. Logistic regression can be seen as a special case of the generalized linear model and thus analogous to linear regression. Deviance is analogous to the sum of squares calculations in linear regression and is a measure of the lack of fit to the data in a logistic regression model. One may begin to understand logistic regression by first considering a logistic model with given parameters, then seeing how the coefficients can be estimated from data. When a \"saturated\" model is available (a model with a theoretically perfect fit), deviance is calculated by comparing a given model with the saturated model. In order to estimate the parameters of such a logistic model and compute how well it fits the data, one must do a logistic regression. Like other forms of regression analysis, logistic regression makes use of one or more predictor variables that may be either continuous or categorical. Then This model has a separate latent variable and a separate set of regression coefficients for each possible outcome of the dependent variable.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5LIgccd7DDh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "0b6908e7-bde5-401d-e4a3-a35c05f4a047"
      },
      "source": [
        "Linear_regression = ' '.join(summary_sentences)  \n",
        "print(Linear_regression)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The extension to multiple and/or vector-valued predictor variables (denoted with a capital X) is known as multiple linear regression, also known as multivariable linear regression. In statistics, linear regression is a linear approach to modelling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). Nearly all real-world regression models involve multiple predictors, and basic descriptions of linear regression are often phrased in terms of the multiple regression model. Errors-in-variables models (or \"measurement error models\") extend the traditional linear regression model to allow the predictor variables X to be observed with error. Linear least squares methods include mainly: Linear regression is widely used in biological, behavioral and social sciences to describe possible relationships between variables. The very simplest case of a single scalar predictor variable x and a single scalar response variable y is known as simple linear regression. For example, weighted least squares is a method for estimating linear regression models when the response variables may have different error variances, possibly with correlated errors. In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Another term, multivariate linear regression, refers to cases where y is a vector, i.e., the same as general linear regression. When controlled experiments are not feasible, variants of regression analysis such as instrumental variables regression may be used to attempt to estimate causal relationships from observational data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xc_C_OL7DK6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "0437b28a-d18f-4925-8661-00d5968970c2"
      },
      "source": [
        "Cluster_analysis = ' '.join(summary_sentences)  \n",
        "print(Cluster_analysis)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Typical cluster models include: A \"clustering\" is essentially a set of such clusters, usually containing all objects in the data set. Connectivity-based clustering, also known as hierarchical clustering, is based on the core idea of objects being more related to nearby objects than to objects farther away. Similar to k-means clustering, these \"density attractors\" can serve as representatives for the data set, but mean-shift can detect arbitrary-shaped clusters similar to DBSCAN. Furthermore, hierarchical clustering can be agglomerative (starting with single elements and aggregating them into clusters) or divisive (starting with the complete data set and dividing it into partitions). For example, k-means clustering can only find convex clusters, and many evaluation indexes assume convex clusters. In centroid-based clustering, clusters are represented by a central vector, which may not necessarily be a member of the data set. The following overview will only list the most prominent examples of clustering algorithms, as there are possibly over 100 published clustering algorithms. Clusterings can be roughly distinguished as: There are also finer distinctions possible, for example: As listed above, clustering algorithms can be categorized based on their cluster model. These methods usually assign the best score to the algorithm that produces clusters with high similarity within a cluster and low similarity between clusters. The most appropriate clustering algorithm for a particular problem often needs to be chosen experimentally, unless there is a mathematical reason to prefer one cluster model over another.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n-kSyfY7H87",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "bb5ca6bf-af98-4724-8715-36eef1f06725"
      },
      "source": [
        "Principal_component_analysis = ' '.join(summary_sentences)  \n",
        "print(Principal_component_analysis)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One way to compute the first principal component efficiently is shown in the following pseudo-code, for a data matrix X with zero mean, without ever computing its covariance matrix. Upon convergence, the weight vectors of the K neurons in the hidden layer will form a basis for the space spanned by the first K principal components. Factor analysis is similar to principal component analysis, in that factor analysis also involves linear combinations of variables. Then, we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix. Mean-centering is unnecessary if performing a principal components analysis on a correlation matrix, as the data are already centered after calculating correlations. The full principal components decomposition of X can therefore be given as where W is a p-by-p matrix of weights whose columns are the eigenvectors of XTX. It extends the classic method of principal component analysis (PCA) for the reduction of dimensionality of data by adding sparsity constraint on the input variables. Independent component analysis (ICA) is directed to similar problems as principal component analysis, but finds additively separable components rather than successive approximations. Another way to characterise the principal components transformation is therefore as the transformation to coordinates which diagonalise the empirical sample covariance matrix. One special extension is multiple correspondence analysis, which may be seen as the counterpart of principal component analysis for categorical data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Sjq4EYD7IIE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "9f08bd73-e0f5-4ae9-b865-204721d1bfac"
      },
      "source": [
        "Algorithm = ' '.join(summary_sentences)  \n",
        "print(Algorithm)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Algorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. Greek mathematicians used algorithms in the sieve of Eratosthenes for finding prime numbers, and the Euclidean algorithm for finding the greatest common divisor of two numbers. Simulation of an algorithm: computer (computor) language: Knuth advises the reader that \"the best way to learn an algorithm is to try it . One of the most important aspects of algorithm design is creating an algorithm that has an efficient run-time, also known as its Big O. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, such as the template method pattern and decorator pattern. Because an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Canonical flowchart symbols : The graphical aide called a flowchart, offers a way to describe and document an algorithm (and a computer program of one). While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another, it yields the number \"1\" for their common measure. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVJyQDRq89gJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "749d88a8-02d9-405f-b0fe-a9dc44eec69f"
      },
      "source": [
        "Neural_network = ' '.join(summary_sentences)  \n",
        "print(Neural_network)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thus a neural network is either a biological neural network, made up of real biological neurons, or an artificial neural network, for solving artificial intelligence (AI) problems.  A neural network is a network or circuit of neurons, or in a modern sense, an artificial neural network, composed of artificial neurons or nodes. Artificial intelligence, cognitive modeling, and neural networks are information processing paradigms inspired by the way biological neural systems process data. Research is ongoing in understanding the computational algorithms used in the brain, with some recent biological evidence for radial basis networks and neural backpropagation as mechanisms for processing data. Neural network theory has served both to better identify how the neurons in the brain function and to provide the basis for efforts to create artificial intelligence. On the other hand, the origins of neural networks are based on efforts to model information processing in biological systems. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence. The utility of artificial neural network models lies in the fact that they can be used to infer a function from observations and also to use it. Artificial intelligence and cognitive modeling try to simulate some properties of biological neural networks. One classical type of artificial neural network is the recurrent Hopfield network.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO-PqTCrOW4D",
        "colab_type": "text"
      },
      "source": [
        "**We now summarized 8 Data Science key concepts!**\n",
        "\n"
      ]
    }
  ]
}